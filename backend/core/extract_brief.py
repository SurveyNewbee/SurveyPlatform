import os
import json
import sys
import time
import yaml
from typing import List, Optional, Dict, Any, Literal
from pathlib import Path

from pydantic import BaseModel, field_validator
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables import RunnableLambda

ALLOWED_PRIMARY_METHODOLOGIES = {
    "conjoint",
    "maxdiff",
    "concept_test",
    "discrete_choice",
    "message_test",
    "pricing_study",
    "segmentation",
    "tracking",
    "descriptive",
}

# LangSmith observability
os.environ.setdefault("LANGCHAIN_TRACING_V2", "true")
os.environ["LANGCHAIN_PROJECT"] = "Basics"  # Force override

# Verify LangSmith API key is available
if os.environ.get("LANGCHAIN_TRACING_V2") == "true" and not os.environ.get("LANGCHAIN_API_KEY"):
    print("⚠ Warning: LANGCHAIN_TRACING_V2 is enabled but LANGCHAIN_API_KEY is not set. Tracing will not work.")


class MarketContext(BaseModel):
    """Competitive and category context."""
    client_brand: Optional[str] = None
    competitor_brands: List[str] = []
    category: Optional[str] = None
    market: Optional[str] = None


class StimulusContent(BaseModel):
    """Individual stimulus description from the brief."""
    label: str
    description: Optional[str] = None


class Operational(BaseModel):
    """Practical execution requirements."""
    target_loi_minutes: Optional[int] = None
    fieldwork_mode: Optional[str] = None
    market_specifics: Optional[str] = None
    quality_controls: Optional[List[str]] = None
    constraints: Optional[str] = None


class StimuliDetails(BaseModel):
    stimuli_type: Optional[str] = None
    stimuli_count: Optional[str] = None
    stimuli_format: Optional[str] = None
    stimuli_content: Optional[List[StimulusContent]] = None

    @field_validator("stimuli_type", "stimuli_count", "stimuli_format", mode="before")
    @classmethod
    def normalize_optional_text(cls, value: Any) -> Optional[str]:
        if value is None:
            return None
        text = str(value).strip()
        return text or None


class AttributeLevel(BaseModel):
    """Represents an attribute with its levels for conjoint/attribute testing."""
    attribute_name: str
    level_count: Optional[str] = None
    levels: Optional[List[str]] = None


class QuotaGroup(BaseModel):
    label: str
    min: Optional[int] = None
    max: Optional[int] = None
    proportion: Optional[float] = None


class Quota(BaseModel):
    attribute: str
    type: Literal["hard", "soft"]
    groups: List[QuotaGroup]


class BlueprintSection(BaseModel):
    """Survey section in the blueprint."""
    section_id: str
    section_title: str
    purpose: str
    question_types: List[str]
    estimated_question_count: int
    estimated_minutes: float
    key_constructs: List[str]
    notes: Optional[str] = None


class ExperimentalDesign(BaseModel):
    """Experimental design configuration."""
    design_type: str
    rotation_scheme: Optional[str] = None
    cells: Optional[int] = None
    stimulus_evaluation_battery: List[str] = []


class PipingChain(BaseModel):
    """Piping chain definition."""
    chain_name: str
    description: str


class SurveyBlueprint(BaseModel):
    """Survey blueprint generated by Agent 1."""
    sections: List[BlueprintSection]
    experimental_design: Optional[ExperimentalDesign] = None
    piping_chains: List[PipingChain] = []
    estimated_total_loi_minutes: float
    loi_assessment: Literal["within_target", "at_risk", "over_target", "no_target_specified"]


class StudyDesign(BaseModel):
    """Study design details (no longer includes study_type - moved to top level)."""
    stimuli_details: Optional[StimuliDetails] = None
    exposure_design: Optional[str] = None
    comparison_intent: Optional[str] = None
    respondent_splitting: Optional[str] = None
    attribute_testing: Optional[List[AttributeLevel]] = None

    @field_validator(
        "exposure_design",
        "comparison_intent",
        "respondent_splitting",
        mode="before",
    )
    @classmethod
    def normalize_optional_text(cls, value: Any) -> Optional[str]:
        if value is None:
            return None
        text = str(value).strip()
        return text or None


class MeasurementGuidance(BaseModel):
    measurement_priority: Optional[str] = None
    required_outputs: Optional[List[str]] = None
    segmentation_intent: Optional[str] = None
    benchmarking: Optional[str] = None

    @field_validator("measurement_priority", "segmentation_intent", "benchmarking", mode="before")
    @classmethod
    def normalize_optional_text(cls, value: Any) -> Optional[str]:
        if value is None:
            return None
        text = str(value).strip()
        return text or None


class ProblemFrame(BaseModel):
    decision_stage: Optional[
        Literal[
            "discover",
            "define",
            "design",
            "validate",
            "measure",
            "optimize",
            "unknown",
        ]
    ] = "unknown"
    primary_problem: Optional[
        Literal[
            "unknown_needs",
            "idea_selection",
            "tradeoff_optimization",
            "experience_breakdown",
            "launch_risk",
            "performance_tracking",
            "decline_diagnosis",
            "opinion_measurement",
            "other",
            "unknown",
        ]
    ] = "unknown"
    decision_risk_level: Optional[Literal["high", "medium", "low", "unknown"]] = "unknown"


class BriefExtraction(BaseModel):
    """
    Structured extraction from research brief.
    
    Core fields (required):
        objective: Primary research goal
        target_audience: Who the research targets
        key_dimensions: Array of topics/attributes to measure
    
    Study classification (new):
        study_type: Broad category (e.g., "new_product_development")
        primary_methodology: Specific method (e.g., "conjoint")
        secondary_objectives: Supporting elements (e.g., ["usage_attitudes"])
        skills: Skills to load for generation (e.g., ["conjoint", "pricing-study"])
    
    Optional details:
        study_design: How the study is structured
        measurement_guidance: Analysis priorities
        constraints: Practical limitations
    """
    # Core fields (required)
    objective: str
    target_audience: str
    key_dimensions: List[str]
    
    # Market context
    market_context: Optional[MarketContext] = None
    
    # Sample design
    total_sample_size: Optional[int] = None
    
    # Study classification (new top-level fields)
    study_type: Optional[str] = None
    primary_methodology: Optional[str] = None
    secondary_objectives: List[str] = []
    
    # Survey blueprint (replaces skills)
    survey_blueprint: Optional[SurveyBlueprint] = None
    
    # Optional details
    study_design: Optional[StudyDesign] = None
    measurement_guidance: Optional[MeasurementGuidance] = None
    problem_frame: Optional[ProblemFrame] = None
    quotas: Optional[List[Quota]] = None
    
    # Operational (replaces top-level constraints)
    operational: Optional[Operational] = None

    @field_validator("objective", "target_audience")
    @classmethod
    def validate_non_empty_text(cls, value: str) -> str:
        if not value or not value.strip():
            raise ValueError("must be a non-empty string")
        return value.strip()

    @field_validator("key_dimensions")
    @classmethod
    def validate_key_dimensions(cls, value: List[str]) -> List[str]:
        if not value:
            raise ValueError("must contain at least one item")
        cleaned = [item.strip() for item in value if isinstance(item, str) and item.strip()]
        if not cleaned:
            raise ValueError("must contain at least one non-empty string")
        return cleaned
    
    @field_validator("secondary_objectives")
    @classmethod
    def validate_string_arrays(cls, value: Any) -> List[str]:
        """Ensure arrays are always lists, even if empty."""
        if value is None:
            return []
        if not isinstance(value, list):
            return []
        return [str(item).strip() for item in value if item]

    @field_validator("primary_methodology", mode="before")
    @classmethod
    def normalize_primary_methodology(cls, value: Any) -> str:
        if value is None:
            return "descriptive"
        text = str(value).strip().lower()
        if not text or text not in ALLOWED_PRIMARY_METHODOLOGIES:
            return "descriptive"
        return text
    
    @field_validator("quotas", mode="before")
    @classmethod
    def normalize_quotas(cls, value: Any) -> Optional[List]:
        if value is None:
            return None
        if not isinstance(value, list) or len(value) == 0:
            return None
        return value


def strip_task_plan(text) -> str:
    """Strip TASK_PLAN block before JSON, handling both raw strings and AIMessage."""
    if hasattr(text, 'content'):
        text = text.content
    # Find the first { which starts the JSON
    idx = text.find('{')
    if idx > 0:
        text = text[idx:]
    # Find the last } to trim any trailing text
    ridx = text.rfind('}')
    if ridx >= 0:
        text = text[:ridx + 1]
    return text


class BriefExtractor:
    """Brief extractor with blueprint generation (Agent 1)."""

    def __init__(self, llm=None, model_name: str = "claude-opus-4-6", temperature: float = 1.0):
        """
        Initialize with any LangChain-compatible LLM.

        Args:
            llm: Pre-configured LLM instance (optional). If provided, model_name is ignored.
            model_name: Model identifier if llm not provided (default: claude-opus-4-6)
            temperature: Temperature setting if llm not provided
        """
        if llm is not None:
            self.llm = llm
        else:
            # Use ChatAnthropic for Claude models, ChatOpenAI for OpenAI models
            if model_name.startswith("claude"):
                self.llm = ChatAnthropic(model=model_name, temperature=temperature)
            else:
                self.llm = ChatOpenAI(model=model_name, temperature=temperature)

        self.parser = JsonOutputParser()
        self.prompt = self._load_prompt()
        self.chain = self.prompt | self.llm | RunnableLambda(strip_task_plan) | self.parser

    def _load_prompt(self) -> ChatPromptTemplate:
        """Load prompt template and add parser format instructions."""
        # Path relative to backend directory
        backend_dir = Path(__file__).parent.parent
        prompt_path = backend_dir / "prompt_template.txt"
        
        if not prompt_path.exists():
            raise FileNotFoundError(f"Prompt template not found: {prompt_path}")

        template_text = prompt_path.read_text(encoding="utf-8")
        template_with_format = f"{template_text}\n\n{{format_instructions}}"
        prompt = ChatPromptTemplate.from_template(template_with_format)

        return prompt.partial(
            format_instructions=self.parser.get_format_instructions()
        )

    async def extract_async_stream(self, brief_text: str):
        """
        Async generator for streaming extraction tokens to frontend.
        Accumulates chunks and sends final structured result.
        Yields complete lines of content for SSE.
        """
        # Build streaming chain without parser (get raw LLM output)
        stream_chain = self.prompt | self.llm
        
        display_buffer = ""
        accumulated_content = ""
        
        try:
            # Stream tokens and accumulate full content
            async for chunk in stream_chain.astream({"brief": brief_text}):
                if hasattr(chunk, 'content'):
                    accumulated_content += chunk.content
                    display_buffer += chunk.content
                    
                    # Yield complete lines for display
                    while '\n' in display_buffer:
                        line, display_buffer = display_buffer.split('\n', 1)
                        yield f"data: {json.dumps({'content': line})}\n\n"
            
            # Yield any remaining buffer
            if display_buffer:
                yield f"data: {json.dumps({'content': display_buffer})}\n\n"
            
            # Parse accumulated content into structured format
            cleaned_content = strip_task_plan(accumulated_content)
            result = self.parser.parse(cleaned_content)
            
            # Transform to frontend format (matching non-streaming endpoint)
            extracted_brief = {
                "objectives": [result.get("objective", "")],  # Convert single to array
                "target_audience": result.get("target_audience", ""),
                "topics": result.get("key_dimensions", []),
                "survey_blueprint": result.get("survey_blueprint"),
            }
            
            # Add optional fields if present
            if result.get("total_sample_size"):
                extracted_brief["total_sample_size"] = result["total_sample_size"]
            if result.get("quotas"):
                extracted_brief["quotas"] = result["quotas"]
            if result.get("market_context"):
                extracted_brief["market_context"] = result["market_context"]
            if result.get("study_type"):
                extracted_brief["study_type"] = result["study_type"]
            if result.get("primary_methodology"):
                extracted_brief["primary_methodology"] = result["primary_methodology"]
            if result.get("secondary_objectives"):
                extracted_brief["secondary_objectives"] = result["secondary_objectives"]
            if result.get("operational"):
                extracted_brief["operational"] = result["operational"]
            if result.get("study_design"):
                extracted_brief["study_design"] = result["study_design"]
            if result.get("measurement_guidance"):
                extracted_brief["measurement_guidance"] = result["measurement_guidance"]
            if result.get("problem_frame"):
                extracted_brief["problem_frame"] = result["problem_frame"]
            
            # Send final structured result (ensure proper JSON serialization)
            final_data = json.dumps({'final': extracted_brief, 'done': True}, default=str)
            yield f"data: {final_data}\n\n"
            
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    def extract(self, brief_text: str, stream_output: bool = False) -> Optional[dict]:
        """
        Main extraction method.
        
        Extracts structured brief data and generates survey blueprint.
        """
        if stream_output:
            result = self._extract_streaming(brief_text)
        else:
            result = self._extract_non_streaming(brief_text)
        
        if result and "survey_blueprint" in result:
            blueprint = result["survey_blueprint"]
            if blueprint:
                print(f"Blueprint generated: {len(blueprint.get('sections', []))} sections, "
                      f"LOI: {blueprint.get('estimated_total_loi_minutes')} min, "
                      f"Assessment: {blueprint.get('loi_assessment')}")
        
        return result

    def _extract_streaming(self, brief_text: str) -> Optional[dict]:
        """Stream partial JSON as generated (works with ANY provider)."""
        print("Extracting fields...\n", flush=True)
        result = None
        seen_keys = set()

        try:
            for chunk in self.chain.stream({"brief": brief_text}):
                result = chunk

                if isinstance(chunk, dict):
                    new_keys = set(chunk.keys()) - seen_keys
                    for key in sorted(new_keys):
                        print(f"✓ {key}", flush=True)
                    seen_keys = set(chunk.keys())

            if result:
                print(f"\n✓ Extraction complete ({len(result.keys())} fields)\n", flush=True)

            return result

        except Exception as e:
            print(f"✗ Extraction error: {e}", flush=True)
            return None

    def _extract_non_streaming(self, brief_text: str) -> Optional[dict]:
        """Non-streaming extraction (works with ANY provider)."""
        try:
            print("Extracting fields...", flush=True)
            result = self.chain.invoke({"brief": brief_text})
            print("✓ Extraction complete\n", flush=True)
            return result
        except Exception as e:
            print(f"✗ Extraction error: {e}", flush=True)
            return None


def format_markdown(result: dict) -> str:
    """Format extraction result as readable markdown."""
    lines: List[str] = ["# Research Brief Summary"]

    objective = str(result.get("objective", "")).strip()
    target_audience = str(result.get("target_audience", "")).strip()
    if objective:
        lines.append(f"- **Objective:** {objective}")
    if target_audience:
        lines.append(f"- **Target Audience:** {target_audience}")

    key_dimensions = [item for item in result.get("key_dimensions", []) if isinstance(item, str) and item.strip()]
    if key_dimensions:
        lines.append("- **Key Dimensions:**")
        lines.extend(f"  - {item}" for item in key_dimensions)
    
    # Market Context (NEW)
    market_context = result.get("market_context") or {}
    if isinstance(market_context, dict):
        mc_lines = []
        if market_context.get("client_brand"):
            mc_lines.append(f"- **Client Brand:** {market_context['client_brand']}")
        if market_context.get("competitor_brands"):
            mc_lines.append(f"- **Competitors:** {', '.join(market_context['competitor_brands'])}")
        if market_context.get("category"):
            mc_lines.append(f"- **Category:** {market_context['category']}")
        if market_context.get("market"):
            mc_lines.append(f"- **Market:** {market_context['market']}")
        if mc_lines:
            lines.append("\n## Market Context")
            lines.extend(mc_lines)
    
    # Study classification
    study_type = result.get("study_type")
    primary_methodology = result.get("primary_methodology")
    secondary_objectives = result.get("secondary_objectives", [])
    
    if study_type or primary_methodology:
        lines.append("\n## Study Classification")
        if study_type:
            lines.append(f"- **Study Type:** {study_type}")
        if primary_methodology:
            lines.append(f"- **Primary Methodology:** {primary_methodology}")
        
        # Total Sample Size (NEW)
        total_sample_size = result.get("total_sample_size")
        if total_sample_size:
            lines.append(f"- **Total Sample Size:** n={total_sample_size}")
        
        if secondary_objectives:
            lines.append(f"- **Secondary Objectives:** {', '.join(secondary_objectives)}")

    study_design = result.get("study_design") or {}
    if isinstance(study_design, dict):
        study_lines: List[str] = []
        
        # Stimuli details (nested)
        stimuli_details = study_design.get("stimuli_details") or {}
        if isinstance(stimuli_details, dict):
            stimuli_lines: List[str] = []
            for label, key in [
                ("Stimuli Type", "stimuli_type"),
                ("Stimuli Count", "stimuli_count"),
                ("Stimuli Format", "stimuli_format"),
            ]:
                value = stimuli_details.get(key)
                if isinstance(value, str) and value.strip():
                    stimuli_lines.append(f"  - **{label}:** {value.strip()}")
            
            # NEW: Stimuli content
            stimuli_content = stimuli_details.get("stimuli_content")
            if stimuli_content and isinstance(stimuli_content, list):
                stimuli_lines.append("  - **Stimuli Content:**")
                for stimulus in stimuli_content:
                    if isinstance(stimulus, dict):
                        label = stimulus.get("label", "N/A")
                        desc = stimulus.get("description", "")
                        if desc:
                            stimuli_lines.append(f"    - {label}: {desc}")
                        else:
                            stimuli_lines.append(f"    - {label}")
            
            if stimuli_lines:
                study_lines.append("- **Stimuli Details:**")
                study_lines.extend(stimuli_lines)
        
        # Other study design fields
        for label, key in [
            ("Exposure Design", "exposure_design"),
            ("Comparison Intent", "comparison_intent"),
            ("Respondent Splitting", "respondent_splitting"),
        ]:
            value = study_design.get(key)
            if isinstance(value, str) and value.strip():
                study_lines.append(f"- **{label}:** {value.strip()}")
        
        # Handle attribute_testing as structured list
        attribute_testing = study_design.get("attribute_testing")
        if attribute_testing and isinstance(attribute_testing, list):
            study_lines.append(f"- **Attribute Testing:** {len(attribute_testing)} attributes")
            for attr in attribute_testing:
                if isinstance(attr, dict):
                    attr_name = attr.get("attribute_name", "")
                    level_count = attr.get("level_count", "")
                    levels = attr.get("levels", [])
                    
                    level_info = f" ({level_count} levels)" if level_count else ""
                    study_lines.append(f"  - **{attr_name}**{level_info}")
                    
                    if levels and isinstance(levels, list):
                        for level in levels:
                            study_lines.append(f"    - {level}")
        
        if study_lines:
            lines.append("\n## Study Design")
            lines.extend(study_lines)

    measurement_guidance = result.get("measurement_guidance") or {}
    if isinstance(measurement_guidance, dict):
        guidance_lines: List[str] = []
        for label, key in [
            ("Measurement Priority", "measurement_priority"),
            ("Segmentation Intent", "segmentation_intent"),
            ("Benchmarking", "benchmarking"),
        ]:
            value = measurement_guidance.get(key)
            if isinstance(value, str) and value.strip():
                guidance_lines.append(f"- **{label}:** {value.strip()}")
        
        # Handle required_outputs as array (UPDATED)
        value = measurement_guidance.get("required_outputs")
        if isinstance(value, list) and value:
            guidance_lines.append(f"- **Required Outputs:** {', '.join(value)}")
        elif isinstance(value, str) and value.strip():
            guidance_lines.append(f"- **Required Outputs:** {value.strip()}")
        
        if guidance_lines:
            lines.append("\n## Measurement Guidance")
            lines.extend(guidance_lines)

    # Operational Requirements (NEW - replaces constraints)
    operational = result.get("operational") or {}
    if isinstance(operational, dict):
        op_lines = []
        if operational.get("target_loi_minutes"):
            op_lines.append(f"- **Target LOI:** {operational['target_loi_minutes']} minutes")
        if operational.get("fieldwork_mode"):
            op_lines.append(f"- **Fieldwork Mode:** {operational['fieldwork_mode']}")
        if operational.get("market_specifics"):
            op_lines.append(f"- **Market Specifics:** {operational['market_specifics']}")
        if operational.get("quality_controls"):
            op_lines.append(f"- **Quality Controls:** {', '.join(operational['quality_controls'])}")
        if operational.get("constraints"):
            op_lines.append(f"- **Constraints:** {operational['constraints']}")
        if op_lines:
            lines.append("\n## Operational Requirements")
            lines.extend(op_lines)
    
    # Quotas
    quotas = result.get("quotas")
    if quotas and isinstance(quotas, list):
        lines.append("\n## Quotas")
        for quota in quotas:
            if isinstance(quota, dict):
                attr = quota.get("attribute", "")
                qtype = quota.get("type", "")
                lines.append(f"- **{attr}** ({qtype})")
                for group in quota.get("groups", []):
                    if isinstance(group, dict):
                        label = group.get("label", "")
                        parts = []
                        if group.get("min") is not None:
                            parts.append(f"min: {group['min']}")
                        if group.get("max") is not None:
                            parts.append(f"max: {group['max']}")
                        if group.get("proportion") is not None:
                            parts.append(f"{group['proportion']:.0%}")
                        lines.append(f"  - {label}: {', '.join(parts) if parts else 'no targets specified'}")

    # Survey Blueprint
    survey_blueprint = result.get("survey_blueprint")
    if survey_blueprint and isinstance(survey_blueprint, dict):
        lines.append("\n## Survey Blueprint")
        
        # Sections
        sections = survey_blueprint.get("sections", [])
        if sections:
            lines.append(f"\n**Sections ({len(sections)}):**")
            for section in sections:
                if isinstance(section, dict):
                    lines.append(f"\n- **{section.get('section_title', 'Untitled')}** (`{section.get('section_id', '')}`)")
                    if section.get('purpose'):
                        lines.append(f"  - Purpose: {section['purpose']}")
                    if section.get('estimated_question_count'):
                        lines.append(f"  - Questions: ~{section['estimated_question_count']}")
                    if section.get('estimated_minutes'):
                        lines.append(f"  - LOI: ~{section['estimated_minutes']} min")
                    if section.get('key_constructs'):
                        lines.append(f"  - Constructs: {', '.join(section['key_constructs'])}")
        
        # Experimental Design
        exp_design = survey_blueprint.get("experimental_design")
        if exp_design and isinstance(exp_design, dict):
            lines.append(f"\n**Experimental Design:**")
            if exp_design.get('design_type'):
                lines.append(f"- Design: {exp_design['design_type']}")
            if exp_design.get('rotation_scheme'):
                lines.append(f"- Rotation: {exp_design['rotation_scheme']}")
            if exp_design.get('cells'):
                lines.append(f"- Cells: {exp_design['cells']}")
            if exp_design.get('stimulus_evaluation_battery'):
                lines.append(f"- Battery: {', '.join(exp_design['stimulus_evaluation_battery'])}")
        
        # Piping Chains
        piping_chains = survey_blueprint.get("piping_chains", [])
        if piping_chains:
            lines.append(f"\n**Piping Chains ({len(piping_chains)}):**")
            for chain in piping_chains:
                if isinstance(chain, dict):
                    lines.append(f"- **{chain.get('chain_name', '')}**: {chain.get('description', '')}")
        
        # LOI Summary
        total_loi = survey_blueprint.get("estimated_total_loi_minutes")
        loi_assessment = survey_blueprint.get("loi_assessment")
        if total_loi or loi_assessment:
            lines.append(f"\n**LOI Estimate:**")
            if total_loi:
                lines.append(f"- Total: {total_loi} minutes")
            if loi_assessment:
                lines.append(f"- Assessment: {loi_assessment}")

    return "\n".join(lines).strip()


def get_survey_template_vars(result: Dict[str, Any]) -> Dict[str, Any]:
    """
    Map extraction result to survey prompt template variables.
    
    Args:
        result: The extraction result dictionary
        
    Returns:
        Dictionary of variables for the survey builder prompt
    """
    return {
        # Existing core fields
        "objective": result.get("objective"),
        "target_audience": result.get("target_audience"),
        "key_dimensions": result.get("key_dimensions"),
        "study_type": result.get("study_type"),
        "primary_methodology": result.get("primary_methodology"),
        "secondary_objectives": result.get("secondary_objectives", []),
        
        # NEW: Survey blueprint (replaces skills)
        "survey_blueprint": result.get("survey_blueprint"),
        
        # NEW: Market context fields
        "client_brand": (result.get("market_context") or {}).get("client_brand"),
        "competitor_brands": (result.get("market_context") or {}).get("competitor_brands", []),
        "category": (result.get("market_context") or {}).get("category"),
        "market": (result.get("market_context") or {}).get("market"),
        
        # NEW: Total sample size
        "total_sample_size": result.get("total_sample_size"),
        
        # Existing stimuli fields + NEW stimuli_content
        "stimuli_type": ((result.get("study_design") or {}).get("stimuli_details") or {}).get("stimuli_type"),
        "stimuli_count": ((result.get("study_design") or {}).get("stimuli_details") or {}).get("stimuli_count"),
        "stimuli_format": ((result.get("study_design") or {}).get("stimuli_details") or {}).get("stimuli_format"),
        "stimuli_content": ((result.get("study_design") or {}).get("stimuli_details") or {}).get("stimuli_content"),
        
        # Existing design fields
        "exposure_design": (result.get("study_design") or {}).get("exposure_design"),
        "comparison_intent": (result.get("study_design") or {}).get("comparison_intent"),
        "respondent_splitting": (result.get("study_design") or {}).get("respondent_splitting"),
        "attribute_testing": (result.get("study_design") or {}).get("attribute_testing"),
        
        # Existing measurement guidance + NEW benchmarking
        "measurement_priority": (result.get("measurement_guidance") or {}).get("measurement_priority"),
        "required_outputs": (result.get("measurement_guidance") or {}).get("required_outputs"),
        "segmentation_intent": (result.get("measurement_guidance") or {}).get("segmentation_intent"),
        "benchmarking": (result.get("measurement_guidance") or {}).get("benchmarking"),
        
        # NEW: Operational fields (replaces top-level constraints)
        "target_loi_minutes": (result.get("operational") or {}).get("target_loi_minutes"),
        "fieldwork_mode": (result.get("operational") or {}).get("fieldwork_mode"),
        "market_specifics": (result.get("operational") or {}).get("market_specifics"),
        "quality_controls": (result.get("operational") or {}).get("quality_controls"),
        "constraints": (result.get("operational") or {}).get("constraints"),
        
        # Existing quotas
        "quotas": result.get("quotas"),
    }


def stream_text(text: str, delay: float = 0.01):
    """Print text character by character to simulate streaming."""
    for char in text:
        print(char, end="", flush=True)
        time.sleep(delay)
    print()  # New line at the end


if __name__ == "__main__":
    brief_text = Path("sample_brief.txt").read_text(encoding="utf-8")

    # Initialize extractor (default: Claude Sonnet 4.5)
    extractor = BriefExtractor()

    # Example for other models:
    # extractor = BriefExtractor(model_name="gpt-4o")
    # or pass a pre-configured LLM:
    # from langchain_openai import ChatOpenAI
    # extractor = BriefExtractor(llm=ChatOpenAI(model="gpt-4o"))

    # Extract with streaming
    result = extractor.extract(brief_text, stream_output=True)

    if result:
        Path("brief_output.json").write_text(
            json.dumps(result, indent=2),
            encoding="utf-8"
        )

        print("\n=== Markdown Output ===")
        markdown_text = format_markdown(result)
        stream_text(markdown_text, delay=0.01)