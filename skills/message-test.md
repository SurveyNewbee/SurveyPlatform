name: message-claim-testing
description: |
  Evaluates the effectiveness of marketing messages, product claims, value propositions, and taglines through systematic measurement of persuasion, credibility, relevance, differentiation, and emotional resonance. Use when selecting between message alternatives, substantiating claims for regulatory or marketing purposes, optimising communication strategy, or validating positioning before launch. Supports both monadic (one message per respondent) and sequential monadic (multiple messages, rotated order) designs. Requires careful attention to competitive context, claim specificity, and diagnostic depth beyond simple preference ranking. Not suitable for testing visual creative executions (use ad-creative-pretest instead) or full concepts with pricing (use concept-test instead).
category: communication_testing
foundational: false
primary_use_case: Identify the most effective messages and claims to drive purchase consideration, brand preference, and communication cut-through
secondary_applications:
  - Claim substantiation for regulatory compliance
  - Value proposition optimisation
  - Tagline and headline testing
  - Competitive message positioning analysis
  - Brand promise validation
  - Communication hierarchy development
  - Pack copy optimisation
commonly_combined_with:
  - concept-test
  - pricing-study
  - competitive-positioning
  - ad-creative-pretest
  - brand-tracking
requires:
  - screening
  - demographics
problem_frames_solved:
  - launch_risk
  - communication_optimisation
  - competitive_strategy
decision_stages:
  - define
  - validate
  - optimise
study_types:
  - communication_testing
  - new_product_development
  - brand_strategy
not_suitable_for:
  - Visual creative execution testing (use ad-creative-pretest)
  - Full product concepts with pricing (use concept-test)
  - Brand equity or awareness measurement (use brand-tracking)
  - Messages that cannot be meaningfully evaluated in text form alone
  - Copy testing where tone/voice matters more than content (requires qualitative research)

---

# Message & Claim Testing Methodology

## Overview

Message and Claim Testing evaluates **which messages, claims, value propositions, or communication elements are most effective** at driving target audience response. It measures **persuasion, credibility, relevance, uniqueness, emotional resonance, and clarity** to identify winning messages and diagnose why some messages work better than others.

This methodology supports both **message selection** (which message wins?) and **message optimisation** (how can we make this message stronger?). It works for product claims, brand value propositions, taglines, headlines, pack copy, and any text-based communication element.

---

## Core Principles

- **Test messages, not creative**
  This methodology tests the *content* of a message — what is being said — not how it looks. Messages are presented as text on a neutral stimulus card. If visual execution matters, use ad-creative-pretest.

- **Competitive context is essential**
  Messages don't exist in a vacuum. Respondents must understand the category and competitive alternatives for credibility and differentiation to be meaningful.

- **Specificity over generality**
  Specific, concrete claims outperform vague ones in testing. Ensure messages being tested are as specific as the final communication will be. "Reduces fine lines by 40% in 2 weeks" tests differently from "Helps reduce fine lines."

- **Monadic or sequential monadic design**
  Each respondent evaluates messages individually, not side-by-side. Side-by-side comparison produces ranking but not absolute performance data. Sequential monadic (each respondent sees 2-4 messages in rotated order) balances statistical power with respondent burden.

- **Diagnose, don't just rank**
  A message that ranks #1 on overall appeal but #4 on credibility needs different treatment than one that ranks #1 on credibility but #4 on emotional resonance. Diagnostic metrics explain *why* messages win or lose.

- **Claim substantiation requires rigour**
  If results will support regulatory claims (e.g., "9 out of 10 dentists recommend"), the methodology must meet regulatory standards for sample size, question wording, and statistical significance.

- **Order effects are real**
  In sequential monadic designs, the first message seen is evaluated differently from the last. Rotation and balanced design are non-negotiable.

- **Emotional and rational work together**
  Effective messages work on both dimensions. Measure both. A message that is rationally compelling but emotionally flat will underperform in market.

---

## Survey Design Requirements

### Question Structure

Surveys must follow this **mandatory testing sequence**:

1. **Category context and need state**
2. **Message exposure** (monadic or sequential monadic)
3. **Core metrics battery** (per message)
4. **Comparative preference** (sequential monadic only)
5. **Open-end diagnostics** (per message)
6. **Brand fit** (if brand attribution is relevant)
7. **Communication hierarchy** (optional — for optimising message combinations)
8. **Demographics**

---

### Section 1: Category Context and Need State

Establish respondent's relationship to the category before showing messages. This grounds their evaluation in real needs.

```
1. How often do you purchase or use [CATEGORY] products?
   - Daily or almost daily
   - Several times a week
   - About once a week
   - A few times a month
   - Once a month or less
   - Rarely / only occasionally
   - Never [SCREEN OUT unless testing category expansion]

2. Which of the following are important to you when choosing [CATEGORY]? Select your top 3.
   [List of category-relevant attributes/benefits — randomise order]
   - [Attribute 1]
   - [Attribute 2]
   - [Attribute 3]
   - [Attribute 4]
   - [Attribute 5]
   - [Attribute 6]
   - Other (please specify)

3. What brand(s) of [CATEGORY] do you currently use? Select all that apply.
   [Brand list — randomise order]
   - [Brand A]
   - [Brand B]
   - [Competitor list]
   - Other (please specify)
   - I don't have a preferred brand
```

**Why this matters:** Need-state priming ensures respondents evaluate messages against their actual priorities, not abstract preferences. Attribute importance enables cross-referencing: does the winning message align with the most important purchase drivers?

---

### Section 2: Message Exposure

#### Design Options

**Monadic design** (recommended for 5+ messages or claim substantiation):
- Each respondent sees ONE message only
- Larger sample required (n=150-200+ per message)
- Eliminates all order effects
- Best for: regulatory claims, absolute performance benchmarking

**Sequential monadic design** (recommended for 2-4 messages):
- Each respondent sees 2-4 messages in rotated order
- Smaller per-message sample needed (n=200-300 total)
- Must use **balanced rotation** (every message appears in every position equally)
- Best for: message selection, relative comparison

#### Message Stimulus Card

Each message must be presented on a **standardised stimulus card**:

```
┌──────────────────────────────────────┐
│                                      │
│  [BRAND NAME] (if brand-attributed)  │
│                                      │
│  ─────────────────────────────────   │
│                                      │
│  "[MESSAGE TEXT]"                     │
│                                      │
│  ─────────────────────────────────   │
│                                      │
│  [Supporting detail if applicable]   │
│                                      │
└──────────────────────────────────────┘
```

**Requirements:**
- Neutral visual design (white/light background, standard font)
- Message text is the only variable — all other card elements are identical across messages
- Message should be readable in 5-15 seconds (20-50 words typical)
- For longer claims, bold the key claim and provide supporting text below
- If testing brand-attributed messages, brand name is shown consistently
- For unbranded message testing, no brand name is shown (isolates message effect)

---

### Section 3: Core Metrics Battery (Per Message)

After each message, measure the following metrics. This battery is asked for EVERY message the respondent sees.

#### 3a. Overall Appeal

```
Overall, how appealing is this message to you?
- Extremely appealing
- Very appealing
- Somewhat appealing
- Not very appealing
- Not at all appealing
```

#### 3b. Persuasion / Purchase Motivation

```
After reading this message, how much more or less likely would you be to consider purchasing [PRODUCT/BRAND]?
- Much more likely to consider
- Somewhat more likely to consider
- No change — neither more nor less likely
- Somewhat less likely to consider
- Much less likely to consider
```

**Note:** This is a *shift* measure (more/less likely), not an absolute purchase intent question. This isolates the message's persuasive effect from baseline category interest.

#### 3c. Credibility / Believability

```
How believable is this message?
- Completely believable
- Very believable
- Somewhat believable
- Not very believable
- Not at all believable
```

#### 3d. Relevance

```
How relevant is this message to you personally?
- Extremely relevant
- Very relevant
- Somewhat relevant
- Not very relevant
- Not at all relevant
```

#### 3e. Uniqueness / Differentiation

```
How different or unique is this message compared to what other [CATEGORY] brands say?
- Completely unique — I haven't heard anything like this before
- Mostly unique — similar to a few things I've heard
- Somewhat unique — I've heard similar things
- Not very unique — many brands say something like this
- Not at all unique — every brand says this
```

#### 3f. Clarity

```
How easy is this message to understand?
- Extremely easy to understand
- Very easy to understand
- Somewhat easy to understand
- Not very easy to understand
- Not at all easy to understand
```

#### 3g. Emotional Response (optional but recommended for hero skills)

```
Which of the following best describes how this message makes you feel? Select up to 2.
- Excited / enthusiastic
- Confident / reassured
- Curious / intrigued
- Trusting
- Indifferent / neutral
- Sceptical / doubtful
- Confused
- Annoyed / put off
- Other (please specify)
```

**Why emotional response matters:** Emotional resonance predicts in-market communication performance better than rational metrics alone. A message that scores well on credibility and relevance but produces "indifferent" emotional response will underperform in market.

---

### Section 4: Comparative Preference (Sequential Monadic Only)

After all messages have been individually evaluated, show them side by side for a forced preference:

```
You've now seen [2/3/4] different messages. Thinking about all of them:

Which ONE message would make you MOST likely to consider purchasing [PRODUCT/BRAND]?

○ Message A: "[Short label or first ~10 words]"
○ Message B: "[Short label or first ~10 words]"
○ Message C: "[Short label or first ~10 words]"
○ Message D: "[Short label or first ~10 words]"
○ None of these would influence my decision
```

**Optional: forced rank**
```
Please rank these messages from most persuasive (1) to least persuasive ([N]).
[Drag-and-drop or numbered assignment]
```

**Why this matters:** Monadic ratings can produce ties or near-ties. Forced preference breaks ties and reveals the "winner takes all" dynamic that reflects real market conditions where only one message gets the headline.

---

### Section 5: Open-End Diagnostics (Per Message)

For EACH message, capture qualitative feedback. These are essential for optimisation recommendations.

```
1. In your own words, what is the main thing this message is saying?
   [Open text — 1-2 sentences]

2. What, if anything, do you find most appealing or convincing about this message?
   [Open text — 1-2 sentences]

3. What, if anything, do you find least appealing or hard to believe about this message?
   [Open text — 1-2 sentences]
```

**Why open-ends matter:** Closed-ended metrics tell you WHAT works and what doesn't. Open-ends tell you WHY. They are essential for message refinement recommendations and for identifying unexpected interpretations.

**Implementation note:** For sequential monadic with 3-4 messages, open-ends on every message can be fatiguing. Options:
- Ask all three open-ends for the first message, only Q1 (main takeaway) for subsequent messages
- Ask Q2 and Q3 only for the "best" and "worst" messages (determined by the respondent's own ratings)

---

### Section 6: Brand Fit (If Brand-Attributed)

If messages are being tested as branded communications:

```
1. How well does this message fit with what you know about [BRAND]?
   - Fits perfectly — exactly what I'd expect from [BRAND]
   - Fits well — consistent with [BRAND]
   - Fits somewhat — I can see it but it's a stretch
   - Doesn't fit well — unexpected from [BRAND]
   - Doesn't fit at all — inconsistent with [BRAND]

2. After reading this message, my overall impression of [BRAND] is:
   - Much more positive than before
   - Somewhat more positive
   - No change
   - Somewhat more negative
   - Much more negative
```

---

### Section 7: Communication Hierarchy (Optional)

When the goal is not just "which message wins" but "which combination of messages works best," use a MaxDiff or allocation exercise:

**MaxDiff approach:**

```
Of these [3-4] messages, which is MOST persuasive and which is LEAST persuasive?

[Show subset of messages — repeat across 6-8 sets for full coverage]

MOST persuasive: ○    LEAST persuasive: ○
Message A: "[text]"
Message B: "[text]"
Message C: "[text]"
```

**Allocation approach:**

```
Imagine you could use a combination of these messages in your marketing. 
Allocate 100 points across the messages based on how much weight you'd give each:

Message A: [__] points
Message B: [__] points
Message C: [__] points
Message D: [__] points
Total: 100 points
```

---

## Scale Design

- **Appeal, Credibility, Relevance, Clarity:** 5-point unipolar scales, consistently worded "Extremely" to "Not at all"
- **Persuasion:** 5-point bipolar shift scale, "Much more likely" to "Much less likely" with a neutral midpoint
- **Uniqueness:** 5-point scale with descriptive anchors (not just "Extremely unique" to "Not at all unique" — use the descriptive wording shown above to make the construct concrete)
- **Emotional response:** Multi-select checklist (max 2 selections), not a scale
- **Comparative preference:** Single-select forced choice
- **Brand fit:** 5-point scale from "Fits perfectly" to "Doesn't fit at all"
- **Open-ends:** Free text, 1-2 sentence guidance, no character limit

**Consistency rules:**
- All 5-point scales use the same positive-to-negative direction
- Scale labels are consistent across messages (same scale for Message A and Message B)
- Randomise message order in sequential monadic (balanced Latin square rotation)
- Do NOT randomise the order of metrics within the battery (keep the same flow for each message to reduce cognitive load)

---

## Common Mistakes to Avoid

- **Testing messages that aren't truly different**
  *Wrong:* Testing "High quality ingredients" vs "Premium quality ingredients"
  *Correct:* Ensure messages differ on at least one meaningful dimension (benefit, proof point, emotional territory, target need)
  *Why it matters:* Near-identical messages produce statistically indistinguishable results, wasting sample

- **Too many messages in sequential monadic**
  *Wrong:* 6+ messages per respondent in sequential monadic
  *Correct:* Maximum 4 per respondent. Use monadic cells for 5+.
  *Why it matters:* Fatigue degrades evaluation quality, and order effects become unmanageable

- **Missing the "compared to what?" context**
  *Wrong:* "Is this message credible?" without competitive reference
  *Correct:* Establish what competitors say (through need-state questions) before evaluating credibility and uniqueness
  *Why it matters:* A message that seems unique to a researcher may be table stakes to a category user

- **Confusing message testing with creative testing**
  *Wrong:* Using message testing methodology for visual ad executions, packaging designs, or video content
  *Correct:* Use ad-creative-pretest for anything where visual execution influences evaluation
  *Why it matters:* Message testing isolates content; creative testing evaluates execution. Mixing them up produces unactionable results.

- **No open-ends**
  *Wrong:* Relying entirely on closed-ended scales
  *Correct:* Always include open-end diagnostics, at minimum "main takeaway" and "most/least appealing"
  *Why it matters:* Without open-ends, you know which message won but not why — and you can't make optimisation recommendations

- **Side-by-side comparison without monadic ratings first**
  *Wrong:* Showing all messages together and asking respondents to rank
  *Correct:* Evaluate each message individually first, then compare
  *Why it matters:* Side-by-side comparison without individual evaluation produces ranking without diagnostics. You know the order but not the absolute quality of each message.

- **Ignoring "clarity" as a diagnostic**
  *Wrong:* Assuming respondents understand the message as intended
  *Correct:* Always measure clarity and check open-end takeaways against intended message
  *Why it matters:* A message that tests well on appeal but is misunderstood will not perform in market. Open-end Q1 ("what is the main thing this message is saying?") is the reality check.

- **Treating all metrics equally**
  *Wrong:* Creating a single composite score by averaging all metrics
  *Correct:* Weight metrics by their predictive power for in-market success: persuasion and appeal typically get highest weight; credibility is a gate (if it fails, other metrics are moot)
  *Why it matters:* Different metrics predict different outcomes. A highly appealing but incredible message fails differently from a credible but boring one.

---

## Analysis & Output Requirements

### Message Scorecard

For each message, produce a scorecard showing all metrics as top-2-box percentages:

```
Message A: "Reduces fine lines by 40% in just 2 weeks"

Metric              Top-2-Box %    Rank (of N)    vs. Average
─────────────────────────────────────────────────────────────
Overall Appeal         62%            1st           +8 pts
Persuasion (shift)     54%            1st           +10 pts
Credibility            38%            3rd           -4 pts
Relevance              58%            2nd           +6 pts
Uniqueness             44%            2nd           +2 pts
Clarity                72%            1st           +4 pts
─────────────────────────────────────────────────────────────
Forced Preference      34%            1st

Key Emotional Response: Excited (28%), Sceptical (22%)

Open-End Themes:
+ "Like the specific timeframe and percentage"
+ "Feels results-oriented and measurable"
- "Sounds too good to be true"
- "40% is suspicious — based on what?"
```

### Diagnostic Profile

Map each message on a 2×2 diagnostic grid:

**Grid 1: Persuasion × Credibility**
- Top-right: Strong persuasion + high credibility = READY TO USE
- Top-left: Strong persuasion + low credibility = NEEDS SUBSTANTIATION
- Bottom-right: Low persuasion + high credibility = BELIEVABLE BUT BORING
- Bottom-left: Low persuasion + low credibility = REJECT

**Grid 2: Relevance × Uniqueness**
- Top-right: High relevance + high uniqueness = DIFFERENTIATED AND RESONANT
- Top-left: High relevance + low uniqueness = TABLE STAKES (everyone says this)
- Bottom-right: Low relevance + high uniqueness = NOVEL BUT NICHE
- Bottom-left: Low relevance + low uniqueness = REJECT

### Segment Analysis

Break all metrics by key segments:
- Demographics (age, gender, income)
- Category usage (heavy vs light users)
- Brand users (current users of target brand vs competitors)
- Need state (which attributes they prioritise)

Identify messages that win among specific segments even if they don't win overall.

### Open-End Coding

Code open-end responses into themes:
- Positive themes (what resonates)
- Negative themes (what doesn't work)
- Takeaway accuracy (does the respondent's interpretation match the intended message?)
- Unexpected interpretations (unintended meanings)

### Statistical Testing

- Test significance of differences between messages on all metrics (at p<0.05)
- For sequential monadic, test for order effects and position bias
- For monadic, test between-cell demographic equivalence
- Flag any message pair where the difference is not statistically significant — report as "statistically tied"

### Communication Hierarchy (if MaxDiff or allocation included)

Produce a scaled importance score for each message (0-100) showing relative persuasive power. Use this to recommend primary, secondary, and supporting messages in a communication hierarchy.

---

## Integration with Other Methods

- **Concept Testing:** Message testing often follows concept testing. Test the concept first, then optimise the messages that will communicate the winning concept.
- **Pricing Research:** Winning messages can be incorporated into pricing stimuli to test whether message framing affects willingness-to-pay.
- **Ad/Creative Pre-Testing:** Message testing identifies WHAT to say; creative pre-testing evaluates HOW it's said in a specific execution.
- **Competitive Positioning:** Message performance can be cross-referenced with competitive attribute ratings to identify messaging white space.
- **Brand Tracking:** Winning messages can be tracked over time for in-market awareness, attribution, and wear-out.

---

## Deliverables Framework

### Primary Outputs

1. **Message Ranking** — Overall winner and runner-up with statistical significance
2. **Message Scorecards** — All metrics per message with benchmarking
3. **Diagnostic Profiles** — 2×2 grids mapping persuasion × credibility and relevance × uniqueness
4. **Segment Winners** — Messages that win among specific target segments
5. **Optimisation Recommendations** — How to strengthen the winning message(s) based on open-end themes and diagnostic gaps

### Secondary Outputs

6. **Communication Hierarchy** — Primary, secondary, and supporting message recommendations
7. **Claim Substantiation Data** — Credibility and takeaway accuracy data for regulatory or legal use
8. **Competitive Message Gap Analysis** — Where the tested messages sit vs competitive communications
9. **Emotional Response Profile** — Emotional resonance patterns by message
10. **Brand Fit Assessment** — Whether messages strengthen or dilute brand perception

---

## Quality Checklist

### Survey Design
- [ ] Category context and need state established before message exposure
- [ ] Messages are presented on standardised, neutral stimulus cards
- [ ] Monadic or sequential monadic design is appropriate for message count
- [ ] Sequential monadic uses balanced rotation (Latin square)
- [ ] Core metrics battery is complete (appeal, persuasion, credibility, relevance, uniqueness, clarity)
- [ ] Open-end diagnostics capture takeaway, positives, and negatives
- [ ] Forced preference question is included (sequential monadic)
- [ ] Brand fit is measured (if brand-attributed messages)
- [ ] Sample size is sufficient per message (n=150+ monadic, n=200+ sequential monadic total)

### Analysis
- [ ] Top-2-box percentages calculated for all metrics
- [ ] Statistical significance tested between all message pairs
- [ ] Order effects tested in sequential monadic
- [ ] Diagnostic grids produced (persuasion × credibility, relevance × uniqueness)
- [ ] Open-ends coded into actionable themes
- [ ] Takeaway accuracy assessed (intended vs perceived message)
- [ ] Segment-level analysis completed
- [ ] Communication hierarchy developed (if MaxDiff/allocation included)

### Deliverables
- [ ] Clear winning message identified with statistical evidence
- [ ] Diagnostic explanation of WHY the winner won (and others lost)
- [ ] Actionable optimisation recommendations for top messages
- [ ] Segment-specific recommendations where relevant
- [ ] Claim substantiation data provided in required format (if regulatory use)

---

## Final Guidance

Message testing is most valuable when it goes beyond "which message wins?" to answer "how do we make our communication as effective as possible?" The ranking is the starting point, not the endpoint.

Key principles for maximum value:

1. **Test meaningful differences.** Messages that differ on a single word produce indistinguishable results. Ensure messages represent genuinely different strategic territories.
2. **Diagnose, don't just rank.** The diagnostic profile (persuasion × credibility, relevance × uniqueness) tells you what to fix. A highly persuasive but incredible message needs proof points, not a rewrite.
3. **Read the open-ends.** Quantitative metrics identify winners; open-ends explain why. The best message optimisation recommendations come from open-end themes.
4. **Respect segment differences.** A message that wins overall may lose among your most valuable segment. Always check segment-level performance before finalising.
5. **Connect to the decision.** The output isn't a research report — it's a message the brand will use in market. Ensure recommendations are specific enough to brief a creative team.

**Remember:** A message test is successful when the marketing team walks away knowing exactly what to say, why it works, and how to make it even stronger.