---
name: naming-testing
description: |
  Evaluates product, brand, or feature names based on clarity, category fit, differentiation, and commercial risk through structured name-only testing. Use when selecting between 3-8 name options during new product development, rebrands, or line extensions where name confusion could impact market success. Tests names in isolation before creative context to identify misinterpretation and negative associations early. Not suitable for creative tagline testing, logo evaluation, or studies where names cannot be separated from visual identity.
category: new_product_development
foundational: false
primary_use_case: Select optimal product or brand names by identifying clear, credible options while eliminating names with confusion or negative association risks
secondary_applications:
  - Risk assessment for existing names under consideration for change
  - Portfolio name rationalization and hierarchy validation
  - Cross-cultural name evaluation for global launches
  - Feature or service naming within established brands
commonly_combined_with:
  - concept-test
  - claims-testing
  - pack-testing
  - brand-architecture
requires:
  - screening
  - rating-scales
problem_frames_solved:
  - idea_selection
  - launch_risk
decision_stages:
  - design
  - validate
study_types:
  - new_product_development
  - brand_strategy
not_suitable_for:
  - Creative tagline or slogan testing (use message-test instead)
  - Logo or visual identity evaluation (use pack-testing)
  - Studies where names cannot be separated from visual context
  - Simple A/B preference testing without diagnostic feedback
---


# Naming Testing Research Methodology

## Overview

Naming Testing research is used to evaluate **product, brand, feature, or service names** to determine which options are **clear, credible, differentiated, and commercially safe**. It is typically conducted during new product development, rebrands, line extensions, or portfolio rationalization.

This methodology prioritizes **risk reduction and decision clarity**. Surveys must surface confusion, misinterpretation, negative associations, and lack of fit early—before names are finalized or legally cleared.

---

## Core Principles

- **Name-first isolation**
  Names must be tested **without logos, taglines, or creative context** unless explicitly testing combined effects.

- **Clarity before appeal**
  A name that is liked but misunderstood is a liability. Comprehension is the primary gate.

- **Interpretation over intention**
  Measure how respondents interpret a name, not what the team intends it to mean.

- **Category anchoring**
  Names must be evaluated relative to the category and competitive set, not in isolation.

- **Risk sensitivity**
  Identify negative, unintended, or culturally sensitive associations early.

- **Comparative discipline**
  Naming decisions are relative choices; tests must enable clear ranking and elimination.

---

## Survey Design Requirements

### Question Structure

Surveys must follow this **mandatory structure** for each name tested:

1. **Name exposure**
2. **Unaided interpretation**
3. **Category fit**
4. **Clarity of offering**
5. **Appeal**
6. **Differentiation**
7. **Credibility**
8. **Negative associations / risk**
9. **Preference and prioritization**
10. **Demographics (if needed)**

#### Name Exposure
- Present the name **exactly as proposed**, including capitalization and spacing.
- Display one name at a time for monadic evaluation, or rotate order in sequential designs.
- Do not explain pronunciation or meaning unless testing pronunciation explicitly.

#### Unaided Interpretation
- Capture what respondents think the name refers to in their own words.
- This must occur before any scaled questions.

#### Category Fit
- Measure whether the name feels appropriate for the category.
- Do not assume category knowledge; verify it.

#### Risk and Associations
- Explicitly probe for negative, confusing, or off-putting reactions.
- This is a required diagnostic, not optional.

---

### Scale Design

- **Clarity**
  - 5-point clarity scale.
  - Anchors: “Very unclear what this is” to “Very clear what this is”.

- **Category Fit**
  - 5-point fit scale.
  - Anchors: “Does not fit this category at all” to “Fits extremely well”.

- **Appeal**
  - 5-point appeal scale.
  - Do not exceed 5 points; higher granularity adds noise.

- **Differentiation**
  - 5-point differentiation scale.
  - Anchors must reference competitors implicitly (“Does not stand out” to “Stands out a lot”).

- **Credibility**
  - 5-point credibility scale.
  - Anchors: “Not at all credible” to “Very credible”.

- **Consistency rules**
  - Use identical scales across all names.
  - Keep polarity consistent.

---

### Sample Questions

**Name Exposure**
> *\[NAME OPTION]*

**Unaided Interpretation**
> Based on this name alone, what do you think this product or service is?

**Category Fit**
> How well does this name fit a \[CATEGORY] offering?  
> - Does not fit at all  
> - Fits slightly  
> - Fits moderately  
> - Fits well  
> - Fits extremely well  

**Negative Associations**
> Does this name bring to mind anything negative, confusing, or off-putting?  
> - Yes (please explain)  
> - No  

**Overall Preference**
> If you had to choose, how would you rate this name overall compared to others you saw?  
> - Much worse  
> - Somewhat worse  
> - About the same  
> - Somewhat better  
> - Much better  

---

## Common Mistakes to Avoid

- **Testing names with creative context**
  *Wrong:* Showing logos or taglines with names  
  *Correct:* Test the name alone first  
  *Why it matters:* Creative can mask weak or confusing names.

- **Skipping interpretation**
  *Wrong:* Asking only liking or appeal  
  *Correct:* Always capture what people think the name means  
  *Why it matters:* Misinterpretation is a hidden failure mode.

- **Over-weighting appeal**
  *Wrong:* Selecting the most liked name  
  *Correct:* Gate on clarity and fit before appeal  
  *Why it matters:* Clever names can be commercially unclear.

- **Ignoring risk feedback**
  *Wrong:* Dismissing minority negative reactions  
  *Correct:* Investigate and understand risk signals  
  *Why it matters:* Naming failures are often low-incidence but high-impact.

- **Testing too many names**
  *Wrong:* 20+ names in one survey  
  *Correct:* 5–8 names per respondent max  
  *Why it matters:* Fatigue reduces discrimination.

---

## Analysis & Output Requirements

The survey must enable the following analyses:

- **Comprehension accuracy**
  - % correctly identifying category or offering
  - Common misinterpretations

- **Name diagnostics**
  - Clarity, fit, appeal, differentiation, credibility scores
  - Identification of strengths and weaknesses per name

- **Risk assessment**
  - Frequency and nature of negative associations
  - Cultural, linguistic, or semantic red flags

- **Comparative ranking**
  - Overall preference ranking
  - Identification of top-tier, mid-tier, and elimination candidates

- **Decision gating**
  - Clear “advance / revise / kill” recommendations
  - Rationale tied to diagnostics, not taste

- **Sample size guidance**
  - Minimum n=100–150 per name for directional reads
  - n=250+ per name for high-stakes decisions
  - Use balanced rotation to manage burden

- **Data structure**
  - One record per respondent per name
  - Clear name identifiers
  - Open-ended interpretation and risk fields linked to name ID

---

## Integration with Other Methods

- **Concept Testing**
  Names should be tested before or alongside concept evaluation.

- **Claims Testing**
  Strong claims cannot compensate for unclear names.

- **Pack Testing**
  Finalist names should be re-tested in packaging context.

- **Brand Architecture**
  Naming decisions must align with portfolio structure and hierarchy.

- **Legal Screening**
  Naming research does not replace trademark or legal clearance, but informs which names are worth clearing.

---

## Quality Checklist

- [ ] Names are tested in isolation, without creative context  
- [ ] Unaided interpretation is captured before scaled measures  
- [ ] Clarity and category fit are primary gating criteria  
- [ ] Appeal is not the sole decision driver  
- [ ] Negative associations and risks are explicitly measured  
- [ ] Scales are consistent across all names  
- [ ] Sample size supports name-level decisions  
- [ ] Analysis enables clear ranking and elimination  
- [ ] Outputs support confident go/no-go decisions  
- [ ] Results integrate cleanly into downstream NPD workflows